{"cells":[{"cellId":"ff7b01b306b14a8186d606d71446b518","cell_type":"markdown","metadata":{"id":"BNUFdbcpOGgY","cell_id":"ff7b01b306b14a8186d606d71446b518","deepnote_cell_type":"markdown"},"source":"# REINFORCE\nIn lecture, you have been introduced to the idea of policy gradient, where the objective is to find a direct mapping from states to actions. In this exercise, you will implement REINFORCE, also commonly referred to as Vanilla Policy Gradient (VPG) method, to solve simple manipulation problems. To finish this exercise, you need to complete the 3 steps listed below:\n\n1. Implement the method to compute the loss of the policy function\n2. Implement the method to compute the loss of the value function\n3. Implement the method to compute the advantage function\n\nThe following code block contains imports, helper functions, and class definitions we've setup for you so you can focus on the most algorithmically interesting parts of REINFORCE. We recommend skimming through it to get a sense for the skeleton shared across most RL setups, but you won't need to implement anything in it.","block_group":"c9e8c397742b40ac8b93d1febba51b21"},{"cellId":"720c33845d2b4ea0a44e15dafb494af9","cell_type":"code","metadata":{"id":"lRBJuPiYOGgY","cell_id":"720c33845d2b4ea0a44e15dafb494af9","deepnote_cell_type":"code"},"source":"import random\nfrom typing import Callable, Dict, Generator, List\n\nimport mpld3\nimport numpy as np\nimport torch\nfrom IPython.display import HTML, display\nfrom matplotlib import pyplot as plt\nfrom pydrake.systems.framework import Context, System\nfrom scipy.signal import lfilter\nfrom torch import nn\nfrom torch.distributions.independent import Independent\nfrom torch.distributions.normal import Normal\nfrom torch.nn import functional as F\n\nfrom manipulation import running_as_notebook\nfrom manipulation.envs.planar_gripper_pushing_a_box import (\n    DrakeGymEnv,\n    PlanarGripperPushingABoxEnv,\n)\nfrom manipulation.exercises.grader import Grader\nfrom manipulation.exercises.rl.test_vpg import TestVPG\n\nif running_as_notebook:\n    mpld3.enable_notebook()\n\n\ndef pad_to_last(\n    nums: np.ndarray, total_length: int, axis: int = -1, val: int = 0\n) -> torch.Tensor:\n    \"\"\"Pad val to last in nums in given axis.\n\n    length of the result in given axis should be total_length.\n\n    Raises:\n      IndexError: If the input axis value is out of range of the nums array\n\n    Args:\n        nums (numpy.ndarray): The array to pad.\n        total_length (int): The final width of the Array.\n        axis (int): Axis along which a sum is performed.\n        val (int): The value to set the padded value.\n\n    Returns:\n        torch.Tensor: Padded array\n\n    \"\"\"\n    tensor = torch.Tensor(nums)\n    axis = (axis + len(tensor.shape)) if axis < 0 else axis\n\n    if len(tensor.shape) <= axis:\n        raise IndexError(\"axis {} is out of range {}\".format(axis, tensor.shape))\n\n    padding_config = [0, 0] * len(tensor.shape)\n    padding_idx = abs(axis - len(tensor.shape)) * 2 - 1\n    padding_config[padding_idx] = max(total_length - tensor.shape[axis], val)\n    return F.pad(tensor, padding_config)\n\n\ndef filter_valids(tensor: torch.Tensor, valids: torch.Tensor) -> list[torch.Tensor]:\n    \"\"\"Filter out tensor using valids (last index of valid tensors).\n\n    valids contains last indices of each rows.\n\n    Args:\n        tensor (torch.Tensor): The tensor to filter\n        valids (torch.Tensor): 1D tensor of shape (N,) where N is the number of rows.\n            Each element valids[i] is the last valid index (exclusive) for tensor[i],\n            i.e., tensor[i][:valids[i]] will be kept.\n\n    Returns:\n        list[torch.Tensor]: Filtered Tensor\n\n    \"\"\"\n    return [tensor[i][:valid] for i, valid in enumerate(valids)]\n\n\ndef discount_cumsum(x: np.ndarray, discount: float) -> np.ndarray:\n    \"\"\"Discounted cumulative sum.\n\n    See https://docs.scipy.org/doc/scipy/reference/tutorial/signal.html#difference-equation-filtering  # noqa: E501\n    Here, we have y[t] - discount*y[t+1] = x[t]\n    or rev(y)[t] - discount*rev(y)[t-1] = rev(x)[t]\n\n    Args:\n        x (np.ndarrary): Input.\n        discount (float): Discount factor.\n\n    Returns:\n        np.ndarrary: Discounted cumulative sum.\n\n\n    \"\"\"\n    return lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n\n\nclass BatchDataset:\n    def __init__(\n        self, inputs: list, batch_size: int, extra_inputs: list | None = None\n    ) -> None:\n        self._inputs = [i for i in inputs]\n        if extra_inputs is None:\n            extra_inputs = []\n        self._extra_inputs = extra_inputs\n        self._batch_size = batch_size\n        if batch_size is not None:\n            self._ids = np.arange(self._inputs[0].shape[0])\n            self.update()\n\n    @property\n    def number_batches(self) -> int:\n        if self._batch_size is None:\n            return 1\n        return int(np.ceil(self._inputs[0].shape[0] * 1.0 / self._batch_size))\n\n    def iterate(self, update: bool = True) -> Generator[list, None, None]:\n        if self._batch_size is None:\n            yield list(self._inputs) + list(self._extra_inputs)\n        else:\n            for itr in range(self.number_batches):\n                batch_start = itr * self._batch_size\n                batch_end = (itr + 1) * self._batch_size\n                batch_ids = self._ids[batch_start:batch_end]\n                batch = [d[batch_ids] for d in self._inputs]\n                yield list(batch) + list(self._extra_inputs)\n            if update:\n                self.update()\n\n    def update(self) -> None:\n        np.random.shuffle(self._ids)\n\n\nclass MLPGaussian(nn.Module):\n    def __init__(\n        self,\n        input_dim: int = 7,\n        output_dim: int = 6,\n        num_hidden: int = 2,\n        hidden_dim: int = 128,\n        nonlinear_act: type[nn.Module] = nn.ReLU,\n        hidden_w_init: Callable = nn.init.xavier_normal_,\n        hidden_b_init: Callable = nn.init.zeros_,\n        output_w_inits: Callable = nn.init.xavier_normal_,\n        output_b_inits: Callable = nn.init.zeros_,\n    ) -> None:\n        super(MLPGaussian, self).__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.fc_in = nn.Linear(input_dim, hidden_dim)\n        self.act = nonlinear_act()\n        self.fc_list = nn.ModuleList()\n        for i in range(num_hidden - 1):\n            hidden_layer = nn.Linear(hidden_dim, hidden_dim)\n            hidden_w_init(hidden_layer.weight)\n            hidden_b_init(hidden_layer.bias)\n            self.fc_list.append(hidden_layer)\n\n        self.fc_out_mean = nn.Linear(hidden_dim, output_dim)\n        self.fc_out_var = nn.Linear(hidden_dim, output_dim)\n\n        output_w_inits(self.fc_out_mean.weight)\n        output_b_inits(self.fc_out_mean.bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        out = self.fc_in(x)\n        out = self.act(out)\n        for _, layer in enumerate(self.fc_list, start=0):\n            out = layer(out)\n            out = self.act(out)\n        out_mean = self.fc_out_mean(out)\n        out_var = self.fc_out_var(out)\n        out_var = self.act(out_var)\n        out_var = out_var + 0.001  # add a small bias to make sure it is not equal to 0\n        return out_mean\n\n\ndef init_path() -> dict:\n    path_sample = {\n        \"observations\": [],\n        \"next_observations\": [],\n        \"actions\": [],\n        \"rewards\": [],\n        \"infos\": [],\n    }\n    return path_sample\n\n\nclass REINFORCE:\n    def __init__(\n        self,\n        env: DrakeGymEnv,\n        policy: \"PolicyEstimator\",\n        value_function: \"ValueEstimator\",\n        util_compute_policy_loss: Callable,\n        util_compute_value_loss: Callable,\n        util_compute_advantage: Callable,\n        use_advantage: bool = False,\n        gae_lambda: float = 1.0,\n        max_episode_length: int = 100,\n        discount_ratio: float = 0.99,\n        learning_rate: float = 0.01,\n    ) -> None:\n        self.env = env\n        self.max_episode_length = max_episode_length\n        self.discount = discount_ratio\n        self._gae_lambda = gae_lambda\n        self.util_compute_policy_loss = util_compute_policy_loss\n        self.util_compute_value_loss = util_compute_value_loss\n        self.util_compute_advantage = util_compute_advantage\n        self.use_advantage = use_advantage\n\n        self.policy = policy\n        self._value_function = value_function\n        self._policy_optimizer = torch.optim.Adam(\n            self.policy.parameters(), lr=learning_rate\n        )\n        self._vf_optimizer = torch.optim.Adam(\n            self._value_function.parameters(), lr=learning_rate\n        )\n\n    def _process_samples(self, paths: list[dict]) -> tuple[\n        torch.Tensor,\n        torch.Tensor,\n        torch.Tensor,\n        torch.Tensor,\n        torch.Tensor,\n        torch.Tensor,\n    ]:\n        r\"\"\"Process sample data based on the collected paths.\n\n        Notes: P is the maximum episode length (self.max_episode_length)\n\n        Args:\n            paths (list[dict]): A list of collected paths\n\n        Returns:\n            torch.Tensor: The observations of the environment\n                with shape :math:`(N, P, O*)`.\n            torch.Tensor: The actions fed to the environment\n                with shape :math:`(N, P, A*)`.\n            torch.Tensor: The acquired rewards with shape :math:`(N, P)`.\n            list[int]: Numbers of valid steps in each paths.\n            torch.Tensor: Value function estimation at each step\n                with shape :math:`(N, P)`.\n\n        \"\"\"\n        valids = torch.Tensor([len(path[\"actions\"]) for path in paths]).int()\n        obs = torch.stack(\n            [\n                pad_to_last(\n                    path[\"observations\"],\n                    total_length=self.max_episode_length,\n                    axis=0,\n                )\n                for path in paths\n            ]\n        )\n        actions = torch.stack(\n            [\n                pad_to_last(\n                    path[\"actions\"],\n                    total_length=self.max_episode_length,\n                    axis=0,\n                )\n                for path in paths\n            ]\n        )\n        rewards = torch.stack(\n            [\n                pad_to_last(path[\"rewards\"], total_length=self.max_episode_length)\n                for path in paths\n            ]\n        )\n        returns = torch.stack(\n            [\n                pad_to_last(\n                    discount_cumsum(path[\"rewards\"], self.discount).copy(),\n                    total_length=self.max_episode_length,\n                )\n                for path in paths\n            ]\n        )\n        with torch.no_grad():\n            baselines = self._value_function(obs)\n\n        return obs, actions, rewards, returns, valids, baselines\n\n    def collect_paths(\n        self, batch_size: int = 100, max_episode_length: int = 100\n    ) -> list[dict]:\n        \"\"\"\n        Args:\n            env: simulation environment (e.g.Gym)\n            batch_size (int, optional): the number of episodes to be included in one patch.\n                                        Defaults to 100.\n            max_episode_length (int, optional): the maximum episode length\n        Returns:\n            [list]: a list of dicts, each dict stores data of one episode\n        \"\"\"\n        env = self.env\n        paths = []\n        invalid_episode_count = 0\n        while len(paths) < batch_size:\n            observation, *_ = env.reset()\n            this_path = init_path()\n            good_episode = True\n\n            for t in range(max_episode_length):\n                # take a step\n                action, _ = self.policy.get_action(observation)\n                observation2, reward, done, truncated, info = env.step(action)\n                if not truncated:\n                    # env.render()\n                    this_path[\"observations\"].append(observation)\n                    this_path[\"next_observations\"].append(observation2)\n                    this_path[\"actions\"].append(action)\n                    this_path[\"rewards\"].append(reward)\n                    this_path[\"infos\"].append(info)\n                else:\n                    # discard this episode if the\n                    # simulation/integration fails\n                    # this is because the time step\n                    # in our simulation\n                    # plant's  is considerably\n                    # large (0.01), in order to\n                    # speed up simulation.\n                    good_episode = False\n                    invalid_episode_count += 1\n                    break\n\n                if done:\n                    break\n                observation = observation2\n            if good_episode:\n                for key in this_path.keys():\n                    this_path[key] = np.c_[this_path[key]]\n                this_path[\"rewards\"] = this_path[\"rewards\"].squeeze()\n                paths.append(this_path)\n        return paths\n\n    def get_mini_batch_data(self, *inputs: torch.Tensor) -> Generator[list, None, None]:\n        \"\"\"mini batch training data generator\"\"\"\n        batch_dataset = BatchDataset(inputs, None)\n        for _ in range(1):\n            for dataset in batch_dataset.iterate():\n                yield dataset\n\n    def train_from_episode_batch(\n        self, itr: int, paths: list[dict]\n    ) -> tuple[float, float, float, float]:\n        \"\"\"Train the algorithm from an episode batch\n\n        Args:\n            itr (int): Iteration number.\n            paths (list[dict]): A list of collected paths.\n\n        Returns:\n            numpy.float64: Calculated mean value of undiscounted returns.\n\n        \"\"\"\n        # the individual paths are not of the same length,\n        # the length of the paths are stored in valids\n\n        (\n            obs,\n            actions,\n            rewards,\n            returns,\n            valids,\n            baselines,\n        ) = self._process_samples(paths)\n        obs_flat = torch.cat(filter_valids(obs, valids))\n        actions_flat = torch.cat(filter_valids(actions, valids))\n        rewards_flat = torch.cat(filter_valids(rewards, valids))\n        returns_flat = torch.cat(filter_valids(returns, valids))\n        advs_flat = self._compute_advantage(rewards, valids, baselines)\n\n        value_loss, policy_loss = 0.0, 0.0\n        if self.use_advantage:\n            for dataset in self.get_mini_batch_data(obs_flat, actions_flat, advs_flat):\n                policy_loss = self._train_policy(*dataset)\n        else:\n            for dataset in self.get_mini_batch_data(\n                obs_flat, actions_flat, returns_flat\n            ):\n                policy_loss = self._train_policy(*dataset)\n\n        for dataset in self.get_mini_batch_data(obs_flat, returns_flat):\n            value_loss = self._train_value_function(*dataset)\n\n        mean_episode_lengths = np.mean(valids.numpy())\n        # undiscounted_returns = list(torch.sum(rewards, dim=1).numpy())\n\n        total_reward = float(float(rewards_flat.sum()))\n        policy_loss, value_loss = policy_loss.item(), value_loss.item()\n\n        print(\n            \"{}, total reward:{}, policy_loss:{}, value_loss:{}, mean episode length:{}\".format(\n                itr,\n                total_reward,\n                policy_loss,\n                value_loss,\n                mean_episode_lengths,\n            )\n        )\n\n        return total_reward, mean_episode_lengths, policy_loss, value_loss\n\n    def _train_policy(\n        self, obs: torch.Tensor, actions: torch.Tensor, returns: torch.Tensor\n    ) -> torch.Tensor:\n        self._policy_optimizer.zero_grad()\n        loss = self.compute_policy_loss(obs, actions, returns)\n        loss.backward()\n        self._policy_optimizer.step()\n        return loss\n\n    def _train_value_function(\n        self, obs: torch.Tensor, returns: torch.Tensor\n    ) -> torch.Tensor:\n        self._vf_optimizer.zero_grad()\n        loss = self.util_compute_value_loss(self._value_function, obs, returns)\n        loss.backward()\n        self._vf_optimizer.step()\n        return loss\n\n    def compute_policy_loss(\n        self, obs: torch.Tensor, actions: torch.Tensor, returns: torch.Tensor\n    ) -> torch.Tensor:\n        \"\"\"call student's implementation\"\"\"\n        loss = self.util_compute_policy_loss(self.policy, obs, actions, returns)\n        return loss\n\n    def _compute_advantage(\n        self, rewards: torch.Tensor, valids: torch.Tensor, baselines: torch.Tensor\n    ) -> torch.Tensor:\n        \"\"\"compute the advantage function\n\n        Args:\n            rewards (torch.tensor): the episodic rewards, batch_size x max episode length\n            valids (torch.tensor): valid episodic lengths\n                    to allow concatenation of episodes of different lengths,\n                    by default, the max episode length is used instead of\n                    the actual episode lengths to construct reward and\n                    baseline tensors\n            baselines (torch.tensor): estimated baselines (state values)\n                                      batch_size x max episode length\n\n        Returns:\n            [torch.tensor]: flattened advantage function\n        \"\"\"\n        advantages = self.util_compute_advantage(\n            self.discount,\n            self._gae_lambda,\n            self.max_episode_length,\n            baselines,\n            rewards,\n        )\n        advantage_flat = torch.cat(filter_valids(advantages, valids))\n\n        return advantage_flat\n\n\ndef draw_training_stats(training_stats: Dict[str, List[float]], title: str) -> None:\n    plt.rcParams.update({\"font.size\": 12})\n    fig, (ax1, ax2, ax3, ax4) = plt.subplots(\n        nrows=4, ncols=1, sharex=True, figsize=(10, 10)\n    )\n    axes = [ax1, ax2, ax3, ax4]\n    ax1.plot(training_stats[\"total_reward\"], \"o-\", label=\"total_rewards\")\n    ax2.plot(training_stats[\"policy_loss\"], \"o-\", label=\"policy_loss\")\n    ax3.plot(training_stats[\"value_loss\"], \"o-\", label=\"value_loss\")\n    ax4.plot(training_stats[\"avg_episode_length\"], \"o-\", label=\"avg_episode_length\")\n    ax1.set_title(title)\n    for ax in axes:\n        ax.legend()\n    axes[-1].set_xlabel(\"# of episodes\")\n\n    ax3.set_yscale(\"log\")\n    ax2.set_yscale(\"log\")\n\n\ndef visualize_policy(\n    reward_function: Callable | None, policy: \"PolicyEstimator | None\" = None\n) -> None:\n    if running_as_notebook:\n        env = PlanarGripperPushingABoxEnv(render=True, reward_function=reward_function)\n        observation, *_ = env.reset()\n        vis = env.simulator.get_system().GetSubsystemByName(\"visualizer\")\n        vis.start_recording()\n        for i in range(10):\n            action = None\n            if policy is None:\n                action = (\n                    np.random.rand(\n                        3,\n                    )\n                    * 0.1\n                    - 0.05\n                )\n            else:\n                action, _ = algo.policy.get_action(observation)\n            observation, *_ = env.step(action)\n\n        vis.stop_recording()\n        ani = vis.get_recording_as_animation(repeat=True)\n        display(HTML(ani.to_jshtml()))","block_group":"e22fde7ccf7b4bfb97edf504d5663343","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"f8b1cbf105494a0ea02cb385a5bc878d","cell_type":"markdown","metadata":{"id":"Q7aRTpchOGgY","cell_id":"f8b1cbf105494a0ea02cb385a5bc878d","deepnote_cell_type":"markdown"},"source":"## Your code starts here!\n\nNow let's check out the manipulation problem you are going to solve! \n\n![Foam block robot gripper setup](https://raw.githubusercontent.com/RussTedrake/manipulation/refs/heads/master/book/figures/exercises/REINFORCE_manipulation_problem.png)\n\n\nThe objective of the problem is to manipulate the foam block in the scene to maximize its $x$ coordinate. We intentionally make this problem extremely simple to help you train and debug faster. If you would like to solve a more difficult task, you are welcome to modify the reward function defined in the cell below. Note that since REINFORCE is an on-policy method, it is not known for being data efficient, so expect longer training time depending on how difficult your task is.","block_group":"2b3ce9169fda4f5ebed3fd6c2db936f1"},{"cellId":"789bdc5e211d4870aad6fa3de6fb4af9","cell_type":"code","metadata":{"id":"i9pqcLB3OGgY","colab":{"height":533,"base_uri":"https://localhost:8080/"},"outputId":"afa1e2bf-ee34-4aaa-a880-a473fa281748","cell_id":"789bdc5e211d4870aad6fa3de6fb4af9","deepnote_cell_type":"code"},"source":"def reward_function(system: System, context: Context) -> float:\n    observation = system.GetOutputPort(\"position\").Eval(context)\n    # observation = [block_x, block_z, block_theta,\n    #                gripper_x, gripper_z, gripper_theta]\n\n    # compute dense rewards\n\n    reward = observation[0]\n    if observation[0] > 0.1:\n        reward = 100.0  # reward goal completion\n    return reward\n\n\nvisualize_policy(reward_function, None)","block_group":"57101b8162a54f87b7bc09e26b214bc2","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"8b3bf4083e1f48919bfad5aa835bdb33","cell_type":"markdown","metadata":{"id":"i4t08PqPA3U9","cell_id":"8b3bf4083e1f48919bfad5aa835bdb33","deepnote_cell_type":"markdown"},"source":"In this exercise, we use the Markov Decision Process (MDP) formulation. We assume we have full access to the poses of the gripper and foam block. In the following sections, we use states and observations interchangeably, both referring to the concatenation of block pose and gripper pose.","block_group":"3f8ab5c79565489483a6a776c415a7af"},{"cellId":"c294aeaecf954719808de826a9761917","cell_type":"markdown","metadata":{"id":"zzN6kklYOGga","cell_id":"c294aeaecf954719808de826a9761917","deepnote_cell_type":"markdown"},"source":"## Review of REINFORCE Algorithm\n\n**REINFORCE** (Monte-Carlo policy gradient) relies on an estimated return by Monte-Carlo methods using episode samples to update the policy parameter $\\theta$. REINFORCE works because the expectation of the sample gradient is equal to the actual gradient:\n\n$$\n\\nabla_\\theta J(\\theta)  = \\mathbb{E}_{\\pi_\\theta} [\\nabla_\\theta \\ln \\pi_\\theta (x_t, u_t) Q^{\\pi_\\theta}(x_t, u_t) ]\n$$\n\n\nwhere $\\pi_\\theta$ is the policy parameterized by $\\theta$, and the action distribution computed from the policy is $\\pi_\\theta(u_t, x_t) = p_{\\pi_\\theta}(u_t \\vert x_t)$. Since $Q^{\\pi_\\theta}(x_t, u_t)$ is unknown, we instead use unbiased samples to approximate it\n\n$$ Q^\\pi(x_t, u_t) = \\mathbb{E}_\\pi[G_t \\vert x_t, u_t] $$\n\nwhere $G_t$ is the returns computed from samples\n\n\nTherefore we can measure $G_t$ from real sample trajectories and use that to update our policy gradient. It relies on a full trajectory and that's why it is a Monte-Carlo method.\n\nThe REINFORCE algorithm is quite straightforward:\n\nInitialize the policy parameter $\\theta$ at random.\n\nGenerate one trajectory on policy $\\pi_\\theta: x_1, u_1, r_2, x_2, u_2, \\dots, x_t$\n\nFor t=1, 2, ..., T:\n\n1. Estimate the the return $G_t= \\sum_{t=0}^{T} \\gamma^t R_t$ where $\\gamma$ is the discount factor.\n\n2. Update policy parameters: $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\ln \\pi_\\theta(x_t, u_t)G_t$, where $\\alpha$ is the learning rate\n\n\nA widely used variation of REINFORCE is to subtract a baseline value from the return $G_t$ to reduce the variance of gradient estimation while keeping the bias unchanged (remember we always want to do this when possible).","block_group":"03d478f53d04488f95a9c351cc48a377"},{"cellId":"8c25a91e65b841508ee7aac70997db53","cell_type":"markdown","metadata":{"id":"N7QF_hV7OGga","cell_id":"8c25a91e65b841508ee7aac70997db53","deepnote_cell_type":"markdown"},"source":"To implement REINFORCE algorithm to solve continuous control tasks, you will inevitably need function approximators to estimate the policy distribution and optionally the value function if an advantage function is used instead of the total discounted rewards for $G_t$. To help you quickly get to the core of the algorithm, we have provided you the implementation of the majority parts of the algorithm. You may find these codes in the `REINFORCE`, `PolicyEstimator`, `ValueEstimator` classes from the setup cell. Note that for this exercise, you are not required to fully understand these codes.","block_group":"ce28634ab13a4125b7ffcbbf6ec96ef7"},{"cellId":"60ac38e85185434ba46d2a06b8ae4bd7","cell_type":"code","metadata":{"id":"PufvcPKIOGga","cell_id":"60ac38e85185434ba46d2a06b8ae4bd7","deepnote_cell_type":"code"},"source":"class PolicyEstimator(nn.Module):\n    def __init__(\n        self,\n        num_hidden: int,\n        hidden_dim: int,\n        obs_dim: int | None = None,\n        action_dim: int | None = None,\n        nonlinear_act: nn.Module = nn.Tanh,\n        init_std: float = 1.0,\n    ) -> None:\n        super(PolicyEstimator, self).__init__()\n        self._obs_dim = obs_dim\n        self._action_dim = action_dim\n\n        self.num_hidden = num_hidden\n        self.hidden_dim = hidden_dim\n        self.model = MLPGaussian(\n            input_dim=self._obs_dim,\n            output_dim=self._action_dim,\n            num_hidden=num_hidden,\n            hidden_dim=hidden_dim,\n            nonlinear_act=nonlinear_act,\n        )\n\n        init_std_param = torch.Tensor([init_std]).log()\n        self._init_log_std = torch.nn.Parameter(init_std_param)\n\n    def predict(self, state: torch.Tensor) -> Independent:\n        state_tensor = state\n        if isinstance(state, np.ndarray):\n            state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n        action_mean = self.model(state_tensor)\n        # post process the action mean\n        # action_mean = torch.sigmoid(action_mean)*0.005 - 0.0025\n        action_std = self._init_log_std.exp() * torch.ones_like(action_mean)\n        action_distribution = Normal(action_mean, action_std)\n        return Independent(action_distribution, 1)\n\n    def forward(\n        self, observations: torch.Tensor\n    ) -> tuple[Independent, Dict[str, torch.Tensor]]:\n        dist = self.predict(observations)\n        return (\n            dist,\n            dict(mean=dist.mean, log_std=(dist.variance**0.5).log()),\n        )\n        # return dist\n\n    def get_action(\n        self, observation: np.ndarray\n    ) -> tuple[np.ndarray, Dict[str, np.ndarray]]:\n        r\"\"\"Get a single action given an observation.\n\n        Args:\n            observation (np.ndarray): Observation from the environment.\n                Shape is :math:`env_spec.observation_space`.\n\n        Returns:\n            tuple:\n                * np.ndarray: Predicted action. Shape is\n                    :math:`env_spec.action_space`.\n                * dict:\n                    * np.ndarray[float]: Mean of the distribution\n                    * np.ndarray[float]: Standard deviation of logarithmic\n                        values of the distribution.\n        \"\"\"\n        if not isinstance(observation, np.ndarray) and not isinstance(\n            observation, torch.Tensor\n        ):\n            observation = self._env_spec.observation_space.flatten(observation)\n        with torch.no_grad():\n            if not isinstance(observation, torch.Tensor):\n                observation = torch.as_tensor(observation).float()\n            observation = observation.unsqueeze(0)\n            action, agent_infos = self.get_actions(observation)\n            return action[0], {k: v[0] for k, v in agent_infos.items()}\n\n    def get_actions(\n        self, observations: np.ndarray\n    ) -> tuple[np.ndarray, Dict[str, np.ndarray]]:\n        r\"\"\"Get actions given observations.\n\n        Args:\n            observations (np.ndarray): Observations from the environment.\n                Shape is :math:`batch_dim \\bullet env_spec.observation_space`.\n\n        Returns:\n            tuple:\n                * np.ndarray: Predicted actions.\n                    :math:`batch_dim \\bullet env_spec.action_space`.\n                * dict:\n                    * np.ndarray[float]: Mean of the distribution.\n                    * np.ndarray[float]: Standard deviation of logarithmic\n                        values of the distribution.\n        \"\"\"\n        if isinstance(observations, list):\n            if isinstance(observations[0], np.ndarray):\n                observations = np.stack(observations)\n            elif isinstance(observations[0], torch.Tensor):\n                observations = torch.stack(observations)\n        with torch.no_grad():\n            if not isinstance(observations, torch.Tensor):\n                observations = torch.as_tensor(observations).float().to()\n            dist, info = self.forward(observations)\n            return dist.sample().cpu().numpy(), {\n                k: v.detach().cpu().numpy() for (k, v) in info.items()\n            }\n\n    def compute_log_likelihood(\n        self, obs: torch.Tensor, actions: torch.Tensor\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            obs (torch.tensor): flattened observations\n            actions (torch.tensor): flattened actions\n        return\n            log_likelihoods (torch.tensor): log probabilities of\n                                          sampling the selected actions\n        \"\"\"\n        distribution = self.predict(obs)\n        return distribution.log_prob(actions)\n\n\nclass ValueEstimator(nn.Module):\n    def __init__(\n        self,\n        num_hidden: int,\n        hidden_dim: int,\n        obs_dim: int | None = None,\n        action_dim: int | None = None,\n        nonlinear_act: nn.Module = nn.Tanh,\n        init_std: float = 1.0,\n    ) -> None:\n        super(ValueEstimator, self).__init__()\n        self._obs_dim = obs_dim\n        self._action_dim = action_dim\n        self.num_hidden = num_hidden\n        self.hidden_dim = hidden_dim\n        self.model = MLPGaussian(\n            input_dim=self._obs_dim,\n            output_dim=1,\n            num_hidden=num_hidden,\n            hidden_dim=hidden_dim,\n            nonlinear_act=nonlinear_act,\n        )\n\n        init_std_param = torch.Tensor([init_std]).log()\n        self._init_log_std = torch.nn.Parameter(init_std_param)\n        self.module = self.model\n\n    def predict(self, state: torch.Tensor) -> Independent:\n        state_tensor = state\n        if isinstance(state, np.ndarray):\n            state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n        value_mean = self.model(state_tensor)\n        value_std = self._init_log_std.exp() * torch.ones_like(value_mean)\n        value_distribution = Normal(value_mean, value_std)\n        return Independent(value_distribution, 1)\n\n    def compute_log_likelihood(\n        self, obs: torch.Tensor, returns: torch.Tensor\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            obs (torch.tensor): flattened observations\n            returns (torch.tensor): flattened a measure of gain\n                                    (e.g.advantage function or discounted rewards)\n        return\n            log_likelihoods (torch.tensor): log probabilities of\n                                          sampling the selected actions\n        \"\"\"\n        dist = self.predict(obs)\n        log_likelihoods = dist.log_prob(returns.reshape(-1, 1))\n        return log_likelihoods\n\n    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n        # torch.Size([246, 1000, 4])\n        dist = self.predict(obs)\n        return dist.mean.flatten(-2)","block_group":"0da13a9e2f6541d1ad6ed4c48e5d1eb2","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"7c8bf92b4a4447268b67faba5538cac6","cell_type":"markdown","metadata":{"id":"SeZ6Wm3EOGga","cell_id":"7c8bf92b4a4447268b67faba5538cac6","deepnote_cell_type":"markdown"},"source":"### (a) Compute Policy Loss\n\nPer the previous section, to train our policy model we want to perform gradient ascent\n\n$$\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\ln \\pi_\\theta(x_t, u_t)G_t$$\n\nThanks to the auto-differentiation feature from PyTorch, we only need to compute the objective value of $ j_t^{\\pi} = \\ln \\pi_\\theta(x_t, u_t)G_t $. From there, simply following the procedure below will automatically compute gradients to optimize all relevant parameters.\n\n```\noptimizer.zero_grad()\nloss = compute_loss(...) # we are at this step\nloss.backward()\noptimizer.step() # by default, minimize loss\n```\n\nEvaluating objective values in batch can effectively reduce the variance in the computed gradients.\n\n$$ J_t^{\\pi} \\leftarrow \\frac{\\sum_{i=1}^N \\ln \\pi_\\theta(x_t, u_t)G_t}{N} $$\n\nwhere $\\ln \\pi_\\theta(x_t, u_t)$ can be obtained from the supplied method `policy.compute_log_likelihood`.\n\nBy default, the PyTorch optimizer minimizes a scalar *loss*. Since in our case we would like to increase the log-likelihoods of generating better actions, the associated loss is\n$$ L_t^{\\pi} \\leftarrow - J_t^{\\pi}  $$\n\nNow in the cell below, implement the `util_compute_policy_loss(policy, obs, actions, returns)` function. Note that `obs`, `actions`, `returns` are all pre-flattened. You may treat them as data in a batch.\n","block_group":"c6a47d989dc24ab9aa3b81a90524ab4b"},{"cellId":"e75c541a12ed40c4bcc58f9bf08f36e4","cell_type":"code","metadata":{"id":"RGpvJHitOGga","cell_id":"e75c541a12ed40c4bcc58f9bf08f36e4","deepnote_cell_type":"code"},"source":"def util_compute_policy_loss(\n    policy: PolicyEstimator,\n    obs: torch.Tensor,\n    actions: torch.Tensor,\n    returns: torch.Tensor,\n) -> torch.Tensor:\n    \"\"\"compute policy loss given observations, actions,\n       and advantages (or total discounted rewards)\n       Note: One can either use the advantage function\n             or discounted returns\n\n    Args:\n        policy (PolicyEstimator): a policy instance\n        obs (torch.tensor): flattened observations Size: data_size, observation dim\n        actions (torch.tensor): flattened actions Size: data_size, action_dim\n        returns (torch.tensor): flattened total discounted returns. Size: data_size\n        advantages (Optional, torch.tensor): flattened advantages\n                                             advantages are not needed here\n\n    Returns:\n        torch.tensor: the mean policy loss, should have size of 1\n    \"\"\"\n    # TODO: compute log likelihoods using the policy's custom function in the codeblock above\n\n    # TODO: get objectives from log likelihoods and returns\n\n    # TODO: average and negate the objectives\n    loss = torch.tensor([0.0])\n    return loss","block_group":"d09dcb7b0dcb4d15bda5306cd328f535","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"68afe5a8c20048e09568d30af765bf4b","cell_type":"markdown","metadata":{"id":"heI_WdygPaEz","cell_id":"68afe5a8c20048e09568d30af765bf4b","deepnote_cell_type":"markdown"},"source":"### (b) Compute Value Loss\n\nWith only the policy loss computed above, you may already run the REINFORCE algorithm as you will see later. To reduce the variance of gradients further, another useful trick is to replace the discounted total rewards with an advantage function $A^{\\pi}$. In this case, the update rule is then\n\n$$\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\ln \\pi_\\theta(x_t, u_t)u_t^{\\pi}$$\n\nAn advantage function intuitively compares the *advantage* of a particular action compared to an *average* action. The value of a particular station-action pair is $Q(x_t, u_t)$, whereas the value of an average action at state $x_t$ is $V(x_t)$. $V(x_t)$ is the value function of the Markov Decision Process (MDP). The estimated state values are also referred to as the baselines.\n\nNow our job is to learn a value function of the MDP problem. It should map from states to scalar values that estimate the average future rewards that can be received from the designated states.\n\n\nLike in part (a), you only need to implement the loss function (not the gradient step itself). You're given the returns (which tells you the training target $V_t$), the observations, and value function, which can give you the log likelihood of the training target given the parameters and other inputs. You want to choose the parameters $\\theta$ that maximizes the log likelihood of the training target:\n$$ \\max_\\theta \\, \\ln p(V_t \\vert u_t, \\theta) $$\nwhere $\\theta$ here refers to the parameters of the value function, and $V_t$ is the training targets stored in `returns`.\n\n**Follow similar steps as in the policy loss, complete the `util_compute_value_loss(value_function, obs, returns)` function below.** Note that you want to return the mean loss accross all observations.","block_group":"3f009d3864074a41912864596bea3a03"},{"cellId":"834ed4fa94d6454f9e69fcd340e88052","cell_type":"code","metadata":{"id":"Mk93nxDqOGga","cell_id":"834ed4fa94d6454f9e69fcd340e88052","deepnote_cell_type":"code"},"source":"def util_compute_value_loss(\n    value_function: ValueEstimator, obs: torch.Tensor, returns: torch.Tensor\n) -> torch.Tensor:\n    \"\"\"compute value function loss given observations and returns\n       Note: One can either use the advantage function\n             or discounted returns\n\n    Args:\n        value_function (ValueEstimator): an instance of a value function\n        obs (torch.tensor): flattened observations\n        returns (torch.tensor): total discounted returns\n\n    Returns:\n        torch.tensor: the mean policy loss, should have size of 1\n    \"\"\"\n    # TODO: compute log likelihoods using the value function's equivalent to part A\n\n    # TODO: average and negate the log likelihoods\n    loss = torch.tensor([0.0])\n    return loss","block_group":"51b61fb810da4f0bbb350e4fa6829155","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"0affdfc00981461fbec2af5242fb9e43","cell_type":"markdown","metadata":{"id":"nilLtBgVPkDg","cell_id":"0affdfc00981461fbec2af5242fb9e43","deepnote_cell_type":"markdown"},"source":"### (c) Compute Advantages\n\nIn this part, we will implement the computation of an advantage function. An advantage function intuitively compares the value of a particular action to that of an average action.\n\nMathematically, we can evaluate the *advantage* of a selected action $u_t$ at $x_t$ by\n\n$$\\delta_t = Q^\\pi(x_t, u_t) - V^\\pi(x_t) =  R(x_t) + \\gamma V'(x_{t+1}) - V'(x_t)$$\n\nwhere $V'(x_t), V'(x_{t+1})$ are evaluated by the value function learned above, $\\gamma$ is the discount factor.\n\nNote $\\delta_t$ is the advantage for a time step. For a trajectory of states and actions, we can compute the discounted sum of advantages similar to rewards.\n\n$$A_t = \\sum_{k=0}^{T-t-1}(\\lambda\\gamma)^k \\delta_{t+k}$$\n\n**In the cell below, compute the advantage function by completing the `compute_advantages` method.**\n\nNote: the baselines are the estimated state values. The rewards are raw rewards, NOT the discounted sums. The shape of the rewards and baselines are both $N \\times T$. This is because an episode may terminate in fewer steps than the maximum episode length. The elements after the termination of episodes are set to 0.","block_group":"816ed8d839a0434c9c09b2044774e537"},{"cellId":"b64fe0791ee049f8b40c19b45d5c0cca","cell_type":"code","metadata":{"id":"CYKlkNrXOGgb","cell_id":"b64fe0791ee049f8b40c19b45d5c0cca","deepnote_cell_type":"code"},"source":"def compute_advantages(\n    discount: float,\n    gae_lambda: float,\n    max_episode_length: int,\n    baselines: torch.Tensor,\n    rewards: torch.Tensor,\n) -> torch.Tensor:\n    \"\"\"Calculate advantages.\n\n    Advantages are a discounted cumulative sum.\n\n    Calculate advantages using a baseline according to Generalized Advantage\n    Estimation (GAE)\n\n    Args:\n        discount (float): RL discount factor (i.e. gamma).\n        gae_lambda (float): Lambda, as used for Generalized Advantage\n            Estimation (GAE).\n        max_episode_length (int): Maximum length of a single episode.\n        baselines (torch.Tensor): A 2D vector of value function estimates with\n            shape (N, T), where N is the batch dimension (number of episodes)\n            and T is the maximum episode length experienced by the agent. If an\n            episode terminates in fewer than T time steps, the remaining\n            elements in that episode should be set to 0.\n        rewards (torch.Tensor): A 2D vector of per-step rewards with shape\n            (N, T), where N is the batch dimension (number of episodes) and T\n            is the maximum episode length experienced by the agent. If an\n            episode terminates in fewer than T time steps, the remaining\n            elements in that episode should be set to 0.\n\n    Returns:\n        torch.Tensor: A 2D vector of calculated advantage values with shape\n            (N, T), where N is the batch dimension (number of episodes) and T\n            is the maximum episode length experienced by the agent. If an\n            episode terminates in fewer than T time steps, the remaining values\n            in that episode should be set to 0.\n\n    \"\"\"\n    # TODO: the following TODOs are intended to guide you through the way\n    # we implemented this, in case you're new to PyTorch. There are multiple\n    # sets of syntax choices that will lead to the correct implementation.\n    # Please feel free to structure your code differently if you'd prefer.\n\n    # TODO: check that the second dimension of your rewards tensor is equal to the maximum length of the episode\n\n    # TODO: create a Nx1 padding vector of ones that you'll reuse in the following lines\n\n    # TODO: create a NxT vector corresponding to V'(x_{t+1})\n    # to do this, select the value function \"baselines\" for the next state and on\n    # pad the missing Nx1 dimension with the padding vector\n\n    # TODO: compute the deltas per the first equation in the cell above\n    # you should use rewards, the shifted \"baselines\" you created, discount, and the original baselines\n\n    # TODO: next, we'll implement the summation from the cell above as a for loop\n    # start by creating an empty tensor of the correct shape to aggregate your advantages\n    advantages = torch.zeros_like(rewards)\n    # TODO: loop over the maximum episode length\n    for t in range(max_episode_length):\n        pass\n        # TODO: create a coefficients vector for your future states\n        # its size is Nx(T - t - 1) and all elements equal the scalar\n        # coefficients that are calculated using the discount and gae_lambda\n        # this vector will correspond to (\\lambda \\gamma)^k\n\n        # TODO: use torch.cumprod to account for the power of k in the summation\n\n        # TODO: pad the beginning of your coefficients with ones (this will account for the k=0 case)\n\n        # TODO: multiply your coefficients by the deltas from t to T\n\n        # TODO: sum this result to get your final summation for the advantage at this step\n\n    return advantages","block_group":"c5e55a96e30347fca2fb3bd4cebfb8ed","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"ffc22cc213b948899207ecb81296d1a0","cell_type":"code","metadata":{"id":"dfZgOm1qOGgb","cell_id":"ffc22cc213b948899207ecb81296d1a0","deepnote_cell_type":"code"},"source":"# make everything deterministic\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\nenv = PlanarGripperPushingABoxEnv(render=False, reward_function=reward_function)\naction_dim = 3\nobs_dim = 6","block_group":"c70e73a58d3a4a758b6e6d85dc2d76ce","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"b858079a8fa142ceba22a6d408a71dd2","cell_type":"markdown","metadata":{"id":"kMi53IATR37n","cell_id":"b858079a8fa142ceba22a6d408a71dd2","deepnote_cell_type":"markdown"},"source":"### REINFORCE without Advantage Function\n\nNow we can compare the performance of the REINFORCE algorithm with and without using the advantage function.","block_group":"b2a3a1d523b6416fa68c0d7db06d7210"},{"cellId":"7ac606f3a0d14de4ba982b10da483f51","cell_type":"code","metadata":{"id":"s43GiJqzOGgb","colab":{"height":741,"base_uri":"https://localhost:8080/"},"outputId":"ca09b51a-9b7f-40e0-9a22-56d4f63e5b71","cell_id":"7ac606f3a0d14de4ba982b10da483f51","deepnote_cell_type":"code"},"source":"policy = PolicyEstimator(\n    action_dim=action_dim,\n    obs_dim=obs_dim,\n    num_hidden=2,\n    hidden_dim=64,\n    nonlinear_act=nn.ReLU,\n)\nvalue_function = ValueEstimator(\n    action_dim=action_dim,\n    obs_dim=obs_dim,\n    num_hidden=2,\n    hidden_dim=64,\n    nonlinear_act=nn.ReLU,\n)\nuse_advantage = False\nprint(\"using advantage: {}\".format(use_advantage))\nalgo = REINFORCE(\n    env=env,\n    policy=policy,\n    value_function=value_function,\n    util_compute_policy_loss=util_compute_policy_loss,\n    util_compute_value_loss=util_compute_value_loss,\n    util_compute_advantage=compute_advantages,\n    use_advantage=use_advantage,\n)\nprint(\"start training\")\ntraining_stats = {\n    \"total_reward\": [],\n    \"policy_loss\": [],\n    \"value_loss\": [],\n    \"avg_episode_length\": [],\n}\nnum_episodes = 50\n\nif running_as_notebook:\n    for i in range(num_episodes):\n        paths = algo.collect_paths(\n            batch_size=20,\n            max_episode_length=10,\n        )\n        (\n            total_reward,\n            mean_episode_lengths,\n            policy_loss,\n            value_loss,\n        ) = algo.train_from_episode_batch(i, paths)\n\n        training_stats[\"total_reward\"].append(total_reward)\n        training_stats[\"policy_loss\"].append(policy_loss)\n        training_stats[\"value_loss\"].append(value_loss)\n        training_stats[\"avg_episode_length\"].append(mean_episode_lengths)\n\n    draw_training_stats(training_stats, \"training_report\")","block_group":"724414bc55a84094bfb2cf77f2fef204","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"4fdec0f7c102488c8ec166b9e50ec36b","cell_type":"code","metadata":{"id":"c5BruT_OOGgb","colab":{"height":533,"base_uri":"https://localhost:8080/"},"outputId":"8e2b033d-ed83-42f4-e8b7-3b80ed6baa69","cell_id":"4fdec0f7c102488c8ec166b9e50ec36b","deepnote_cell_type":"code"},"source":"visualize_policy(reward_function, policy)","block_group":"2544e6412af340989e54d1f578b9ddca","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"9e0b251c7d5f42e080fe6eba9d2eba38","cell_type":"markdown","metadata":{"id":"ljWLRsFJR9nq","cell_id":"9e0b251c7d5f42e080fe6eba9d2eba38","deepnote_cell_type":"markdown"},"source":"### REINFORCE with Advantage Function","block_group":"842fa880463443faa238ddb45e4f37ba"},{"cellId":"2d3ab6c4fca348d8aa447ba00430e0c4","cell_type":"code","metadata":{"id":"g89Mq8r8OGgc","colab":{"height":741,"base_uri":"https://localhost:8080/"},"outputId":"70861381-29aa-40a7-b194-58d51f53d961","cell_id":"2d3ab6c4fca348d8aa447ba00430e0c4","deepnote_cell_type":"code"},"source":"policy = PolicyEstimator(\n    action_dim=action_dim,\n    obs_dim=obs_dim,\n    num_hidden=2,\n    hidden_dim=64,\n    nonlinear_act=nn.ReLU,\n)\nvalue_function = ValueEstimator(\n    action_dim=action_dim,\n    obs_dim=obs_dim,\n    num_hidden=2,\n    hidden_dim=64,\n    nonlinear_act=nn.ReLU,\n)\n\nuse_advantage = True\nprint(\"using advantage: {}\".format(use_advantage))\nalgo = REINFORCE(\n    env=env,\n    policy=policy,\n    value_function=value_function,\n    util_compute_policy_loss=util_compute_policy_loss,\n    util_compute_value_loss=util_compute_value_loss,\n    util_compute_advantage=compute_advantages,\n    use_advantage=use_advantage,\n)\nprint(\"start training\")\ntraining_stats = {\n    \"total_reward\": [],\n    \"policy_loss\": [],\n    \"value_loss\": [],\n    \"avg_episode_length\": [],\n}\nnum_episodes = 50\n\nif running_as_notebook:\n    for i in range(num_episodes):\n        paths = algo.collect_paths(\n            batch_size=20,\n            max_episode_length=10,\n        )\n        (\n            total_reward,\n            mean_episode_lengths,\n            policy_loss,\n            value_loss,\n        ) = algo.train_from_episode_batch(i, paths)\n\n        training_stats[\"total_reward\"].append(total_reward)\n        training_stats[\"policy_loss\"].append(policy_loss)\n        training_stats[\"value_loss\"].append(value_loss)\n        training_stats[\"avg_episode_length\"].append(mean_episode_lengths)\n\n    draw_training_stats(training_stats, \"training_report\")","block_group":"c08417e9acd04afa8ac4e8985c94d672","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"0d494fd895c14f44b88248540ea9dd7d","cell_type":"code","metadata":{"id":"Y1fe5TlpOGgc","colab":{"height":533,"base_uri":"https://localhost:8080/"},"outputId":"bd662849-671c-479d-d8ce-a5aad3ed0f76","cell_id":"0d494fd895c14f44b88248540ea9dd7d","deepnote_cell_type":"code"},"source":"visualize_policy(reward_function, policy)","block_group":"915fcc8a317e449e80468249b90c9033","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"a416cb2d84514247bca85bcfc872d7f7","cell_type":"markdown","metadata":{"id":"LvTF2EBTB3Ur","cell_id":"a416cb2d84514247bca85bcfc872d7f7","deepnote_cell_type":"markdown"},"source":"Now if you have extra time, you may want to modify the reward function to learn more complicated skills. For example, you may learn the box flipping skill from the force control chapter using REINFORCE method!","block_group":"4c0996ba71f742fb9be01e46dc790d57"},{"cellId":"9c2033b72f0041328f1014bef97b7ef5","cell_type":"markdown","metadata":{"id":"3u1YKG0LOGgd","cell_id":"9c2033b72f0041328f1014bef97b7ef5","deepnote_cell_type":"markdown"},"source":"Weng, L. (2018, April 08). Policy Gradient Algorithms. Retrieved November 19, 2020, from https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html","block_group":"5a65c5b4e9444ed9944737e63baf586e"},{"cellId":"7880859419c24036897ab59395601965","cell_type":"markdown","metadata":{"id":"MwE8yNg58VQN","cell_id":"7880859419c24036897ab59395601965","deepnote_cell_type":"markdown"},"source":"## What quantitative effect does using advantage have on our key learning metrics?\n\nRead through the `reward`, `policy_loss`, and `value_loss` values through training and observe the diagrams above. What do you notice?","block_group":"0e963f63c7f74215a4cae7bdce44e1b8"},{"cellId":"3836381ea625419d89a3e90a673b7314","cell_type":"markdown","metadata":{"cell_id":"3836381ea625419d89a3e90a673b7314","deepnote_cell_type":"markdown"},"source":"##### VERIFICATION IN GRADESCOPE (1/2)\n\nWhat is the final `reward`, `policy_loss`, and `value_loss` for REINFORCE *without* the advantage function? \n\n##### VERIFICATION IN GRADESCOPE (2/2)\n\nWhat is the final `reward`, `policy_loss`, and `value_loss` for REINFORCE *with* the advantage function? ","block_group":"2f21582ef5954aae9be97cc6e72bf3d6"}],
        "metadata": {"deepnote_notebook_id":"45cc79fd37c644628bcd2205a550f0d6"},
        "nbformat": "4",
        "nbformat_minor": "0",
        "version": "0"
      }