{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced ICP: soft correspondences with Generalized-ICP\n",
    "\n",
    "Standard ICP and point-to-plane ICP treat all correspondences as hard constraints with equal weight. This makes them sensitive to incorrect matches--a single bad correspondence gets the same influence as a good one.\n",
    "\n",
    "**Generalized-ICP (GICP)** addresses this through *soft correspondences*: matches are weighted by their geometric consistency. The algorithm uses local surface structure from both scans to automatically down-weight unreliable correspondences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "from pydrake.all import (\n",
    "    PointCloud,\n",
    "    Rgba,\n",
    "    RigidTransform,\n",
    "    RollPitchYaw,\n",
    "    RotationMatrix,\n",
    "    StartMeshcat,\n",
    ")\n",
    "\n",
    "from manipulation import FindResource\n",
    "from manipulation.icp import IterativeClosestPoint\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:drake:Meshcat listening for connections at http://localhost:7000\n"
     ]
    }
   ],
   "source": [
    "meshcat = StartMeshcat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford bunny point clouds\n",
    "\n",
    "We'll use the Stanford Bunny, transformed with a known rotation and translation plus noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model shape: (3, 8171)\n",
      "Scene shape: (3, 8171)\n"
     ]
    }
   ],
   "source": [
    "model_pcl = np.load(FindResource(\"models/bunny/bunny.npy\"))\n",
    "\n",
    "X_WO_true = RigidTransform(\n",
    "    RotationMatrix.MakeXRotation(np.pi / 6),\n",
    "    [-0.02, 0.02, 0.02]\n",
    ")\n",
    "\n",
    "scene_pcl = X_WO_true.multiply(model_pcl)\n",
    "np.random.seed(42)\n",
    "scene_pcl += np.random.randn(*scene_pcl.shape) * 0.0005\n",
    "\n",
    "print(f\"Model shape: {model_pcl.shape}\")\n",
    "print(f\"Scene shape: {scene_pcl.shape}\")\n",
    "\n",
    "cloud = PointCloud(model_pcl.shape[1])\n",
    "cloud.mutable_xyzs()[:] = model_pcl\n",
    "\n",
    "meshcat.Delete()\n",
    "meshcat.SetProperty(\"/Background\", \"visible\", False)\n",
    "meshcat.SetProperty(\"/Cameras/default/rotated/<object>\", \"zoom\", 10.5)\n",
    "meshcat.SetObject(\"model\", cloud, point_size=0.01, rgba=Rgba(0, 0, 1))\n",
    "meshcat.SetTransform(\"model\", RigidTransform())\n",
    "\n",
    "cloud2 = PointCloud(scene_pcl.shape[1])\n",
    "cloud2.mutable_xyzs()[:] = scene_pcl\n",
    "meshcat.SetObject(\"scene\", cloud2, point_size=0.01, rgba=Rgba(1, 0, 0))\n",
    "meshcat.SetTransform(\"scene\", RigidTransform())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The probabilistic framework\n",
    "\n",
    "GICP models each point as drawn from a Gaussian: $p_i \\sim N(\\hat{p}_i, C_i)$\n",
    "\n",
    "The covariance $C_i$ encodes local surface structure:\n",
    "- Small variance along the surface normal (we know position accurately)\n",
    "- Large variance in the tangent plane (uncertain about exact position on surface)\n",
    "\n",
    "$$C_i = R \\begin{bmatrix} \\epsilon & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} R^T$$\n",
    "\n",
    "where $R$ aligns with the surface normal and $\\epsilon = 0.001$.\n",
    "\n",
    "GICP then minimizes the Mahalanobis distance:\n",
    "\n",
    "$$T^* = \\arg\\min_T \\sum_i d_i^T (C_i^A + T C_i^B T^T)^{-1} d_i$$\n",
    "\n",
    "When normals are aligned, the combined covariance stays thin (strong constraint). When normals differ, it becomes isotropic (weak constraint). This automatic reweighting is the key to robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing surface normals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_normals(points, k_neighbors=20):\n",
    "    \"\"\"Estimate surface normals using local neighborhoods.\"\"\"\n",
    "    kdtree = KDTree(points.T)\n",
    "    normals = np.zeros_like(points)\n",
    "    k = min(k_neighbors, points.shape[1])\n",
    "    \n",
    "    for i in range(points.shape[1]):\n",
    "        # Find k nearest neighbors for each point\n",
    "        _, indices = kdtree.query(points[:, i], k=k)\n",
    "        if np.isscalar(indices):\n",
    "            indices = [indices]\n",
    "        \n",
    "        # Fit a plane through the neighbors - normal is the eigenvector \n",
    "        # corresponding to the smallest eigenvalue\n",
    "        neighbors = points[:, indices]\n",
    "        centered = neighbors - np.mean(neighbors, axis=1, keepdims=True)\n",
    "        cov = centered @ centered.T\n",
    "        \n",
    "        eigvalues, eigvecs = np.linalg.eigh(cov)\n",
    "        normals[:, i] = eigvecs[:, 0] / np.linalg.norm(eigvecs[:, 0])\n",
    "    \n",
    "    return normals\n",
    "\n",
    "\n",
    "print(\"Computing surface normals...\")\n",
    "model_normals = compute_normals(model_pcl)\n",
    "scene_normals = compute_normals(scene_pcl)\n",
    "print(f\"Done: {model_normals.shape[1]} model normals, {scene_normals.shape[1]} scene normals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing covariance matrices\n",
    "\n",
    "Each point's covariance matrix encodes the local surface geometry. We construct it by:\n",
    "\n",
    "1. **Building a local coordinate frame**: Given a surface normal $\\mathbf{n}_i$, we construct an orthonormal basis $[\\mathbf{n}_i, \\mathbf{t}_1, \\mathbf{t}_2]$ where $\\mathbf{t}_1$ and $\\mathbf{t}_2$ are tangent vectors in the surface plane.\n",
    "\n",
    "2. **Defining anisotropic uncertainty**: In this local frame, we assign small variance $\\varepsilon$ along the normal direction (high confidence) and unit variance in the tangent directions (low confidence):\n",
    "\n",
    "$$\\Lambda = \\begin{bmatrix} \\varepsilon & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}, \\quad \\varepsilon \\ll 1$$\n",
    "\n",
    "3. **Transforming to world coordinates**: The covariance matrix in world coordinates is:\n",
    "\n",
    "$$C_i = R_i \\Lambda R_i^T$$\n",
    "\n",
    "where $R_i = [\\mathbf{n}_i \\mid \\mathbf{t}_1 \\mid \\mathbf{t}_2]$ is the rotation matrix formed by the orthonormal basis.\n",
    "\n",
    "This construction ensures that the uncertainty is:\n",
    "- **Minimal** along the normal direction (we know the point lies *on* the surface)\n",
    "- **Maximal** in the tangent plane (we're uncertain about the *exact* position along the surface)\n",
    "\n",
    "The ratio $1/\\varepsilon$ determines how strongly the surface orientation constrains the alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_covariances(points, normals, epsilon=0.001):\n",
    "    \"\"\"Build covariance matrices that encode local surface geometry.\"\"\"\n",
    "    covariances = []\n",
    "    \n",
    "    for i in range(points.shape[1]):\n",
    "        n = normals[:, i] / np.linalg.norm(normals[:, i])\n",
    "        \n",
    "        # Build orthonormal basis [n, t1, t2] aligned with the surface\n",
    "        # Start with an arbitrary vector that's not parallel to n\n",
    "        if abs(n[0]) < 0.9:\n",
    "            v = np.array([1, 0, 0])\n",
    "        else:\n",
    "            v = np.array([0, 1, 0])\n",
    "        \n",
    "        # Gram-Schmidt to get tangent vectors\n",
    "        t1 = v - (v @ n) * n\n",
    "        t1 = t1 / np.linalg.norm(t1)\n",
    "        t2 = np.cross(n, t1)\n",
    "        \n",
    "        # Construct covariance: small variance along normal, large in tangent plane\n",
    "        R = np.column_stack([n, t1, t2])\n",
    "        C_local = np.diag([epsilon, 1.0, 1.0])\n",
    "        C = R @ C_local @ R.T\n",
    "        \n",
    "        covariances.append(C)\n",
    "    \n",
    "    return covariances\n",
    "\n",
    "model_covariances = compute_covariances(model_pcl, model_normals)\n",
    "scene_covariances = compute_covariances(scene_pcl, scene_normals)\n",
    "\n",
    "# Verify the anisotropy - should see epsilon and two 1.0s\n",
    "eigvals = np.linalg.eigvalsh(model_covariances[100])\n",
    "print(f\"Example eigenvalues: {eigvals} (small/large ratio = {eigvals[2]/eigvals[0]:.0f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized ICP implementation\n",
    "\n",
    "GICP solves a weighted least-squares problem at each iteration. Given current estimate $T$ (rotation $R$, translation $\\mathbf{t}$), point correspondences, and covariance matrices, we seek an incremental update $\\Delta T$ that minimizes:\n",
    "\n",
    "$$E(T) = \\sum_{i=1}^{N} \\mathbf{d}_i^T (C_i^M + C_{c_i}^S)^{-1} \\mathbf{d}_i$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{d}_i = R\\mathbf{p}_i^M + \\mathbf{t} - \\mathbf{p}_{c_i}^S$ is the residual between transformed model point and its scene correspondence\n",
    "- $C_i^M$ is the covariance of model point $i$ (transformed to world frame: $R C_i^M R^T$)\n",
    "- $C_{c_i}^S$ is the covariance of the corresponding scene point\n",
    "- $(C_i^M + C_{c_i}^S)^{-1}$ is the information matrix (inverse covariance) that weights each correspondence\n",
    "\n",
    "### Linearization\n",
    "\n",
    "We parameterize the incremental transformation using:\n",
    "- **Rotation update**: axis-angle representation $\\boldsymbol{\\omega} \\in \\mathbb{R}^3$ \n",
    "- **Translation update**: $\\boldsymbol{\\Delta t} \\in \\mathbb{R}^3$\n",
    "\n",
    "For small updates, the transformed point linearizes as:\n",
    "\n",
    "$$T(\\mathbf{p}) \\approx \\mathbf{p} + [\\boldsymbol{\\omega}]_\\times \\mathbf{p} + \\boldsymbol{\\Delta t}$$\n",
    "\n",
    "where $[\\boldsymbol{\\omega}]_\\times$ is the skew-symmetric matrix:\n",
    "\n",
    "$$[\\boldsymbol{\\omega}]_\\times = \\begin{bmatrix} 0 & -\\omega_z & \\omega_y \\\\ \\omega_z & 0 & -\\omega_x \\\\ -\\omega_y & \\omega_x & 0 \\end{bmatrix}$$\n",
    "\n",
    "### Jacobian\n",
    "\n",
    "The Jacobian of the residual with respect to the 6-DOF update parameters is:\n",
    "\n",
    "$$J_i = \\frac{\\partial \\mathbf{d}_i}{\\partial [\\boldsymbol{\\omega}^T, \\boldsymbol{\\Delta t}^T]^T} = \\begin{bmatrix} -[\\mathbf{p}_i]_\\times & I_{3\\times3} \\end{bmatrix} \\in \\mathbb{R}^{3 \\times 6}$$\n",
    "\n",
    "where $[\\mathbf{p}_i]_\\times$ captures how rotation around each axis moves the point, and $I$ is the identity (translation directly adds to position).\n",
    "\n",
    "### Weighted least-squares solution\n",
    "\n",
    "We convert the problem to standard least-squares form by applying the Cholesky decomposition $C^{-1} = L L^T$:\n",
    "\n",
    "$$\\min_{\\mathbf{x}} \\sum_i \\| L_i (J_i \\mathbf{x} + \\mathbf{d}_i) \\|^2$$\n",
    "\n",
    "Solving the normal equations $J^T W J \\mathbf{x} = -J^T W \\mathbf{d}$ gives us the optimal update $\\mathbf{x} = [\\boldsymbol{\\omega}^T, \\boldsymbol{\\Delta t}^T]^T$, which we convert back to a rigid transformation and compose with the current estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalized_icp(model_pts, scene_pts, model_cov, scene_cov, \n",
    "                   initial_transform, max_iterations=30, tolerance=1e-6):\n",
    "    \"\"\"Run GICP with soft correspondences weighted by covariance.\"\"\"\n",
    "    T = initial_transform\n",
    "    errors = []\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Transform model points with current estimate\n",
    "        model_transformed = T.multiply(model_pts)\n",
    "        \n",
    "        # Find nearest neighbors in the scene\n",
    "        kdtree_scene = KDTree(scene_pts.T)\n",
    "        dists, idxs = kdtree_scene.query(model_transformed.T)\n",
    "        correspondences = scene_pts[:, idxs]\n",
    "        \n",
    "        # Build weighted least-squares system\n",
    "        A = []\n",
    "        b = []\n",
    "        total_error = 0\n",
    "        \n",
    "        R_mat = T.rotation().matrix()\n",
    "        \n",
    "        for i in range(model_transformed.shape[1]):\n",
    "            p_model_tf = model_transformed[:, i]\n",
    "            p_scene_corresp = correspondences[:, i]\n",
    "            \n",
    "            # Combined covariance (transform model cov to world frame first)\n",
    "            C_model_world = R_mat @ model_cov[i] @ R_mat.T\n",
    "            C_combined = C_model_world + scene_cov[idxs[i]] + np.eye(3) * 1e-6\n",
    "            C_inv = np.linalg.inv(C_combined)\n",
    "            \n",
    "            # Residual and Mahalanobis distance\n",
    "            r = p_model_tf - p_scene_corresp\n",
    "            total_error += r.T @ C_inv @ r\n",
    "            \n",
    "            # Jacobian: derivatives w.r.t. [omega; delta_t]\n",
    "            # Rotation part uses skew-symmetric matrix of the point\n",
    "            p_cross = np.array([\n",
    "                [0, -p_model_tf[2], p_model_tf[1]],\n",
    "                [p_model_tf[2], 0, -p_model_tf[0]],\n",
    "                [-p_model_tf[1], p_model_tf[0], 0]\n",
    "            ])\n",
    "            \n",
    "            J = np.hstack([-p_cross, np.eye(3)])\n",
    "            \n",
    "            # Weight by sqrt of information matrix (Cholesky)\n",
    "            try:\n",
    "                L = np.linalg.cholesky(C_inv)\n",
    "            except:\n",
    "                # Fallback if not positive definite\n",
    "                eigvals, eigvecs = np.linalg.eigh(C_inv)\n",
    "                L = eigvecs @ np.diag(np.sqrt(np.maximum(eigvals, 1e-10))) @ eigvecs.T\n",
    "            \n",
    "            A.append(L @ J)\n",
    "            b.append(-L @ r)\n",
    "        \n",
    "        # Solve normal equations: (A^T A) x = A^T b\n",
    "        A_mat = np.vstack(A)\n",
    "        b_vec = np.hstack(b)\n",
    "        \n",
    "        x = np.linalg.solve(A_mat.T @ A_mat + np.eye(6) * 1e-8, A_mat.T @ b_vec)\n",
    "        omega = x[:3]  # axis-angle rotation\n",
    "        t_update = x[3:]  # translation\n",
    "        \n",
    "        # Convert axis-angle to rotation matrix\n",
    "        angle = np.linalg.norm(omega)\n",
    "        if angle > 1e-10:\n",
    "            axis = omega / angle\n",
    "            K = np.array([\n",
    "                [0, -axis[2], axis[1]],\n",
    "                [axis[2], 0, -axis[0]],\n",
    "                [-axis[1], axis[0], 0]\n",
    "            ])\n",
    "            R_inc = np.eye(3) + np.sin(angle) * K + (1 - np.cos(angle)) * (K @ K)\n",
    "            R_inc_drake = RotationMatrix(R_inc)\n",
    "        else:\n",
    "            R_inc_drake = RotationMatrix.Identity()\n",
    "        \n",
    "        # Compose the update with current transform\n",
    "        T = RigidTransform(R_inc_drake, t_update).multiply(T)\n",
    "        \n",
    "        avg_error = total_error / model_transformed.shape[1]\n",
    "        errors.append(avg_error)\n",
    "        \n",
    "        # Check convergence\n",
    "        if iteration > 0 and abs(errors[-2] - errors[-1]) < tolerance:\n",
    "            break\n",
    "        if np.linalg.norm(omega) < 1e-6 and np.linalg.norm(t_update) < 1e-6:\n",
    "            break\n",
    "    \n",
    "    return T, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: accuracy with small initialization error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guess = RigidTransform(\n",
    "    RotationMatrix.MakeXRotation(np.pi / 6 + 0.15),\n",
    "    [-0.015, 0.017, 0.024]\n",
    ")\n",
    "\n",
    "X_init_error = initial_guess.inverse().multiply(X_WO_true)\n",
    "print(f\"Initial guess error: {np.linalg.norm(X_init_error.translation())*1000:.1f} mm, \"\n",
    "      f\"{abs(RollPitchYaw(X_init_error.rotation()).vector()[0])*180/np.pi:.1f} deg\\n\")\n",
    "\n",
    "T_gicp, _ = generalized_icp(model_pcl, scene_pcl, model_covariances, scene_covariances, initial_guess)\n",
    "T_standard, _ = IterativeClosestPoint(p_Om=model_pcl, p_Ws=scene_pcl, X_Ohat=initial_guess, max_iterations=25)\n",
    "\n",
    "gicp_error = np.linalg.norm(T_gicp.inverse().multiply(X_WO_true).translation())\n",
    "std_error = np.linalg.norm(T_standard.inverse().multiply(X_WO_true).translation())\n",
    "\n",
    "print(f\"\\nStandard ICP error: {std_error*1000:.3f} mm\")\n",
    "print(f\"GICP error:         {gicp_error*1000:.3f} mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: robustness to noisy initialization\n",
    "\n",
    "The key advantage of GICP is robustness. Let's test with large initialization errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_robustness(num_trials=10, translation_noise=0.1, rotation_noise=0.15):\n",
    "    results = {\"standard\": [], \"gicp\": []}\n",
    "    \n",
    "    print(f\"Testing with {num_trials} trials (noise: +/-{translation_noise*1000:.0f} mm, +/-{rotation_noise*180/np.pi:.0f} deg)\\n\")\n",
    "    \n",
    "    for trial in range(num_trials):\n",
    "        t_noisy = X_WO_true.translation() + np.random.randn(3) * translation_noise\n",
    "        angle_noisy = np.pi / 6 + np.random.randn() * rotation_noise\n",
    "        \n",
    "        noisy_guess = RigidTransform(RotationMatrix.MakeXRotation(angle_noisy), t_noisy)\n",
    "\n",
    "        T_std, _ = IterativeClosestPoint(p_Om=model_pcl, p_Ws=scene_pcl, X_Ohat=noisy_guess, max_iterations=25)\n",
    "        error_std = np.linalg.norm(T_std.inverse().multiply(X_WO_true).translation())\n",
    "        results[\"standard\"].append(error_std)\n",
    "\n",
    "        T_g, _ = generalized_icp(model_pcl, scene_pcl, model_covariances, scene_covariances, noisy_guess, max_iterations=30)\n",
    "        error_g = np.linalg.norm(T_g.inverse().multiply(X_WO_true).translation())\n",
    "        results[\"gicp\"].append(error_g)\n",
    "\n",
    "        print(f\"[Iteration {trial:>2}] ICP error: {error_std*1000:.3f} mm, GICP error: {error_g*1000:.3f} mm\")\n",
    "    \n",
    "    std_success = [e for e in results[\"standard\"] if e < 0.005]\n",
    "    gicp_success = [e for e in results[\"gicp\"] if e < 0.005]\n",
    "    \n",
    "    print(\"Results:\")\n",
    "    print(f\"Standard ICP: {len(std_success)}/{num_trials} succeeded\")\n",
    "    if std_success:\n",
    "        print(f\"  Mean error: {np.mean(std_success)*1000:.3f} +/- {np.std(std_success)*1000:.3f} mm\")\n",
    "    \n",
    "    print(f\"GICP:         {len(gicp_success)}/{num_trials} succeeded\")\n",
    "    if gicp_success:\n",
    "        print(f\"  Mean error: {np.mean(gicp_success)*1000:.3f} +/- {np.std(gicp_success)*1000:.3f} mm\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "results = test_robustness(num_trials=10, translation_noise=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing soft correspondences\n",
    "\n",
    "Let's examine how GICP weights correspondences based on normal alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform model covariances and normals to world frame\n",
    "R_mat = T_gicp.rotation().matrix()\n",
    "model_transformed = T_gicp.multiply(model_pcl)\n",
    "\n",
    "# Find correspondences after GICP convergence\n",
    "kdtree = KDTree(scene_pcl.T)\n",
    "distances, indices = kdtree.query(model_transformed.T)\n",
    "\n",
    "weights = []\n",
    "normal_alignments = []\n",
    "\n",
    "# Sample some correspondences to analyze\n",
    "for i in range(min(500, model_pcl.shape[1])):\n",
    "    C_model_world = R_mat @ model_covariances[i] @ R_mat.T\n",
    "    C_combined = C_model_world + scene_covariances[indices[i]]\n",
    "    \n",
    "    # Weight = 1/smallest eigenvalue (strongest constraint direction)\n",
    "    eigvals = np.linalg.eigvalsh(C_combined)\n",
    "    weights.append(1.0 / eigvals[0])\n",
    "    \n",
    "    # How aligned are the surface normals?\n",
    "    n_model_world = (R_mat @ model_normals[:, i])\n",
    "    n_scene = scene_normals[:, indices[i]]\n",
    "    normal_alignments.append(abs(n_model_world @ n_scene))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(normal_alignments, weights, alpha=0.5, s=10) \n",
    "plt.xlabel(\"Normal alignment |n1 * n2|\", fontsize=12)\n",
    "plt.ylabel(\"Constraint weight (1/lambda_min)\", fontsize=12)\n",
    "plt.title(\"Soft correspondences\", fontsize=13)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(weights, bins=30, alpha=0.7, edgecolor=\"black\")\n",
    "plt.xlabel(\"Constraint weight\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.title(\"Weight distribution\", fontsize=13)\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "corr = np.corrcoef(normal_alignments, weights)[0, 1]\n",
    "print(f\"Correlation between normal alignment and weight: {corr:.3f}\")\n",
    "print(\"(Positive correlation confirms: aligned normals -> stronger constraints)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary.** While both ICP and GICP work well with good initialization, GICP significantly outperforms standard ICP with noisy initialization by assigning higher weights to normals that are aligned.\n",
    "\n",
    "### Why GICP works better\n",
    "\n",
    "The probabilistic framework lets GICP encode surface geometry in covariance matrices. When correspondences are geometrically inconsistent (misaligned normals), they're automatically down-weighted. This makes the algorithm naturally robust to incorrect matches without manual parameter tuning.\n",
    "\n",
    "This tutorial is based on [\"Generalized-ICP\" by Aleksandr V. Segal et al. (2009)](https://www.robots.ox.ac.uk/~avsegal/resources/papers/Generalized_ICP.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
