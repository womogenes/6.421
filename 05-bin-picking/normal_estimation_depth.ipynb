{"cells":[{"cellId":"65e27932b27440b19fc9ad0cef22c0a1","cell_type":"markdown","metadata":{"id":"9CagYlhclDR4","cell_id":"65e27932b27440b19fc9ad0cef22c0a1","deepnote_cell_type":"markdown"},"source":"# Normal Estimation from Depth Image","block_group":"c74cb82ea9364784ad48e63be6bd599a"},{"cellId":"744d4efbea1a42baace6e08df27348eb","cell_type":"code","metadata":{"id":"_GMCWQ1RjBoB","cell_id":"744d4efbea1a42baace6e08df27348eb","deepnote_cell_type":"code"},"source":"from copy import deepcopy\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Rectangle\nfrom pydrake.all import RigidTransform, RotationMatrix, StartMeshcat\n\nfrom manipulation import running_as_notebook\nfrom manipulation.meshcat_utils import AddMeshcatTriad\nfrom manipulation.mustard_depth_camera_example import MustardExampleSystem","block_group":"81ee148a04904407a83080528f250de5","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"7080d6ad9cbe4529bd342078d419b3f1","cell_type":"code","metadata":{"cell_id":"7080d6ad9cbe4529bd342078d419b3f1","deepnote_cell_type":"code"},"source":"# Start the visualizer.\nmeshcat = StartMeshcat()","block_group":"ed8b23e5b717486d8893759fd6212e4f","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"80b9aada6d824337a55db1af24cf21d7","cell_type":"markdown","metadata":{"id":"UxATaRk3jBoH","cell_id":"80b9aada6d824337a55db1af24cf21d7","deepnote_cell_type":"markdown"},"source":"# Problem Description\nIn the lecture, we learned about estimating the point cloud normal vectors and surface curvations. For this exercise, you will investigate a slightly different approach. In particular, you will exploit the structure already presented in a depth image to avoid computing nearest neighbors. \n\nYou will implement the `estimate_normal_by_nearest_pixels` method.\n\nRun the cell below to set up the simulation environment.","block_group":"afa4b4bf148b434287419467ab18e907"},{"cellId":"ab3fc4569ce248a0844224d32f96b101","cell_type":"code","metadata":{"id":"STaJPvHmjBoH","cell_id":"ab3fc4569ce248a0844224d32f96b101","deepnote_cell_type":"code"},"source":"class NormalEstimation:\n    def __init__(self) -> None:\n        diagram = MustardExampleSystem()\n        context = diagram.CreateDefaultContext()\n\n        # setup\n        meshcat.SetProperty(\"/Background\", \"visible\", False)\n\n        # getting data\n        self.point_cloud = diagram.GetOutputPort(\"camera0_point_cloud\").Eval(context)\n        self.rgb_im = diagram.GetOutputPort(\"camera0_rgb_image\").Eval(context).data\n        self.depth_im_read = (\n            diagram.GetOutputPort(\"camera0_depth_image\").Eval(context).data.squeeze()\n        )\n        self.depth_im = deepcopy(self.depth_im_read)\n        self.depth_im[self.depth_im == np.inf] = 10.0\n        label_im = (\n            diagram.GetOutputPort(\"camera0_label_image\").Eval(context).data.squeeze()\n        )\n        self.mask = label_im == 1\n        point_cloud = diagram.GetOutputPort(\"camera0_point_cloud\").Eval(context)\n        cloud = point_cloud.Crop(\n            lower_xyz=[-0.3, -0.3, -0.3], upper_xyz=[0.3, 0.3, 0.3]\n        )\n        meshcat.SetObject(\"point_cloud\", cloud)\n\n        # camera specs\n        cam0 = diagram.GetSubsystemByName(\"camera0\")\n        cam0_context = cam0.GetMyMutableContextFromRoot(context)\n        self.X_WC = cam0.body_pose_in_world_output_port().Eval(cam0_context)\n        self.cam_info = cam0.default_depth_render_camera().core().intrinsics()\n\n        AddMeshcatTriad(meshcat, \"least_squares_basis\", length=0.03, radius=0.0005)\n        meshcat.SetTransform(\n            \"least_squares_basis\",\n            cam0.body_pose_in_world_output_port().Eval(cam0_context),\n        )\n\n    def project_depth_to_pC(self, depth_pixel: np.ndarray, uv=None) -> np.ndarray:\n        \"\"\"\n        project depth pixels to points in camera frame\n        using pinhole camera model\n        Input:\n            depth_pixels: numpy array of (nx3) or (3,)\n        Output:\n            pC: 3D point in camera frame, numpy array of (nx3)\n        \"\"\"\n        # switch u,v due to python convention\n        v = depth_pixel[:, 0]\n        u = depth_pixel[:, 1]\n        Z = depth_pixel[:, 2]\n        # read camera intrinsics\n        cx = self.cam_info.center_x()\n        cy = self.cam_info.center_y()\n        fx = self.cam_info.focal_x()\n        fy = self.cam_info.focal_y()\n        X = (u - cx) * Z / fx\n        Y = (v - cy) * Z / fy\n        pC = np.c_[X, Y, Z]\n        return pC\n\n    def plot_scanning_window(\n        self, u_range: tuple[int, int], v_range: tuple[int, int]\n    ) -> None:\n        \"\"\"\n        visualize the scanning window\n        u_range: (u_start, u_end)\n        v_range: (v_start, v_end)\n        u, v are the 1st and 2nd axis of the image array\n        \"\"\"\n        # switch u, v range to get x, y\n        x0, x1 = v_range\n        y0, y1 = u_range\n        fig, ax = plt.subplots()\n        ax.imshow(self.depth_im)\n        ax.add_patch(Rectangle((x0, y0), x1 - x0, y1 - y0, alpha=0.5, fc=\"r\"))\n\n    def vis_normals(self, normals: list[RigidTransform]) -> None:\n        \"\"\" \"\"\"\n        for i in range(len(normals)):\n            name = \"normal_vec_{}\".format(i)\n            AddMeshcatTriad(meshcat, name, length=0.01, radius=0.001, X_PT=normals[i])\n\n\ndef bbox(img: np.ndarray) -> tuple[tuple[int, int], tuple[int, int]]:\n    a = np.where(img != 0)\n    bbox = ([np.min(a[0]), np.max(a[0])], [np.min(a[1]), np.max(a[1])])\n    return bbox\n\n\nenv = NormalEstimation()\nmask = env.mask\ndepth_im = env.depth_im","block_group":"3f1c5d8631f0454d8a04c6f95f800ad0","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"5a56cf999bf6486a8f035be0207138ed","cell_type":"markdown","metadata":{"id":"SevGI1izjBoN","cell_id":"5a56cf999bf6486a8f035be0207138ed","deepnote_cell_type":"markdown"},"source":"The object of interest is the mustard bottle. Our goal in this exercise is to compute the estimate of point cloud normals of the mustard bottle from a depth image. The depth image is visualized below.","block_group":"6f25b41d54d943b596f6ad04bebe8ed2"},{"cellId":"1e729d0afa3244ca82ca0bff19863f87","cell_type":"code","metadata":{"id":"TyACO5ThjBoO","cell_id":"1e729d0afa3244ca82ca0bff19863f87","deepnote_cell_type":"code"},"source":"plt.imshow(depth_im)","block_group":"8cbd64f7b19642bf892034fbf0f643dd","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"9c4023df07a44a5eab46adabcab1d2f3","cell_type":"markdown","metadata":{"id":"yDpgse_ojBoS","cell_id":"9c4023df07a44a5eab46adabcab1d2f3","deepnote_cell_type":"markdown"},"source":"The core idea of the approach is to exploit the fact that a depth image already includes spatial information among pixels. For example, for a selected pixel, the pixels that surround it are likely to be its nearest neighbors. Therefore, instead of computing nearest neighbors, we can instead use the nearest pixels in place of nearest neighbors. \n\nThe cell below provides a sequence of screenshots of the method, where a square/rectangular window moves across the depth image. All pixels in the sliding window is used to compute the normal vector of the center point of the window. In your implementation below, you will use a smaller window and a smaller step size to get better accuracy.","block_group":"3c61696b4dbd4b0197b22a3df4811f86"},{"cellId":"c6c642d547704f7495cd830a344cee7c","cell_type":"code","metadata":{"id":"WD7U95EDjBoT","cell_id":"c6c642d547704f7495cd830a344cee7c","deepnote_cell_type":"code"},"source":"uv_step = 40\nv_bound, u_bound = bbox(mask)\n\nfor v in range(v_bound[0], v_bound[1], uv_step):\n    for u in range(u_bound[0], u_bound[1], uv_step):\n        center = [v, u]\n        u_length = 30\n        v_length = 30\n        if running_as_notebook:\n            env.plot_scanning_window(\n                [center[0] - v_length, center[0] + v_length + 1],\n                [center[1] - u_length, center[1] + u_length + 1],\n            )","block_group":"1f174a95c24d454fa72f2f705a595778","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"a17ce2ad7bd44d47a67c1a4efe5895b3","cell_type":"markdown","metadata":{"id":"o-8vPWxdjBoX","cell_id":"a17ce2ad7bd44d47a67c1a4efe5895b3","deepnote_cell_type":"markdown"},"source":"## Mapping Depth Image to Point Cloud\n\nNote that pixel indices of a depth image is not a valid position measurement in the 3D world. Fortunately, there is a simple mapping from pixel locations to poses in the 3D world, and it is called the [pinhole camera model](https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html). We have helped you map all pixels of the depth image to points in the camera frame in the cell below. In case you need to gain direct access to this mapping, please refer to the `project_depth_to_pC` method in the `NormalEstimation` class.\n\nThe diagram below is found from [OpenCV documentation](https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html). Note that the $u$ and $v$ directions are reversed in Python due to the difference in convention.","block_group":"7a9f1b9ed93c44bd8221c6bcec056e8a"},{"cellId":"cab0d9dfe4174f9e8d92110593e08e42","cell_type":"markdown","metadata":{"id":"OaXlVRcskbDf","cell_id":"cab0d9dfe4174f9e8d92110593e08e42","deepnote_cell_type":"markdown"},"source":"![](https://docs.opencv.org/3.4/pinhole_camera_model.png)","block_group":"61e4cd5ee1284799ab87729e86fa50ad"},{"cellId":"e63385e9dd864d32a4b85c66292f94c6","cell_type":"code","metadata":{"id":"JHn2SKHDjBoY","cell_id":"e63385e9dd864d32a4b85c66292f94c6","deepnote_cell_type":"code"},"source":"img_h, img_w = depth_im.shape\nv_range = np.arange(img_h)\nu_range = np.arange(img_w)\ndepth_u, depth_v = np.meshgrid(u_range, v_range)\ndepth_pnts = np.dstack([depth_v, depth_u, depth_im])\ndepth_pnts = depth_pnts.reshape([img_h * img_w, 3])\n# point poses in camera frame\npC = env.project_depth_to_pC(depth_pnts)","block_group":"b1ba4f541c3c4681a73cb8ad0fd1379a","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"3722db4879614a9cb2c8fccf3f098ae8","cell_type":"markdown","metadata":{"id":"StDSt_MtjBoc","cell_id":"3722db4879614a9cb2c8fccf3f098ae8","deepnote_cell_type":"markdown"},"source":"## Computing Surface Normals by Nearest Pixels\nNow we should be able to calculate the surface normals. Recall from section 5.5.2 of the textbook, in each sliding window we can use the points to construct a data matrix (also known as a *scatter matrix*) **W** which exhibits special properties that allows us to estimate the local normals. \n\n### Verification 1:\nWhich eigenvector of **W** corresponds to the vector normal to the points in the sliding window? Assume we want to specify a normal frame whose z-axis corresponds to the vector normal to the points in the sliding window. How can you use the eigenvectors of **W** to create a rotation matrix representing the desired orientation of the normal frame?\n\n**Select the best answer in Gradescope.**","block_group":"7dcf4ae5be9846448e434f49d5d9117f"},{"cellId":"e063c6792b354111a4a0d79348091bc7","cell_type":"markdown","metadata":{"cell_id":"e063c6792b354111a4a0d79348091bc7","deepnote_cell_type":"markdown"},"source":"**Complete the implementation of the `estimate_normal_by_nearest_pixels` below.** \n\nNote that locations of sliding windows are provided to you for the ease of grading. The pose of the depth camera is `X_WC`, **it is a different depth camera from the one shown in the meshcat visualizer**. Lastly, **make sure the +z axis of the normal frame points outward, toward the depth camera (different from the one shown in the meshcat visualizer)**. It will be useful to review section 5.5.2 in the notes for computing the normal estimate. \n\nHINT: consider using *np.linalg.eigh*","block_group":"2f2b5bf98fe34ec2a8c146166b38b8b0"},{"cellId":"6fe6fb434cb94199b29a6387c4f9855d","cell_type":"code","metadata":{"id":"arTPxyRMjBod","cell_id":"6fe6fb434cb94199b29a6387c4f9855d","deepnote_cell_type":"code"},"source":"X_WC = env.X_WC\n\n\ndef estimate_normal_by_nearest_pixels(\n    X_WC: RigidTransform, pC: np.ndarray, uv_step: int = 10\n) -> list[RigidTransform]:\n    \"\"\"\n    compute the surface normals from the nearest pixels (by a sliding window)\n    Input:\n        X_WC: RigidTransform of the camera in world frame\n        pC: 3D points computed from the depth image in the camera frame\n        uv_step: recommended step size for the sliding window (see codes below)\n    Output:\n        normals: a list of RigidTransforms of the normal frames in world frame.\n                 The +z axis of the normal frame is the normal vector, it should\n                 points outward (towards the camera)\n    \"\"\"\n    normals = []\n    v_bound, u_bound = bbox(mask)\n    pC = pC.reshape(img_h, img_w, 3)\n    for v in range(v_bound[0], v_bound[1], uv_step):\n        for u in range(u_bound[0], u_bound[1], uv_step):\n            # center of the window at depth_im[u,v]\n            center = [v, u]\n            u_length = 3\n            v_length = 3\n            # side of the window\n            v_range = np.arange(max(v - v_length, 0), min(v + v_length + 1, img_h - 1))\n            u_range = np.arange(max(u - u_length, 0), min(u + u_length + 1, img_w - 1))\n\n            ###################\n            # fill your code here\n            ###################\n\n    return normals","block_group":"0c816520e9a9480b89b210748c053bc5","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"0a0efe99da6d44e3bc71716fa1e6769f","cell_type":"code","metadata":{"id":"PD117pt0jBol","cell_id":"0a0efe99da6d44e3bc71716fa1e6769f","deepnote_cell_type":"code"},"source":"normals = estimate_normal_by_nearest_pixels(X_WC, pC)\nenv.vis_normals(normals)","block_group":"f3d22bc6c88344ecb20274fa5d45a489","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"c2c309d56c8a4edca4d9d7f1e30c7d74","cell_type":"code","metadata":{"id":"xWaeAZOwlz5-","cell_id":"c2c309d56c8a4edca4d9d7f1e30c7d74","deepnote_cell_type":"code"},"source":"# Use this to test your implementation if you want!\nfrom manipulation.exercises.clutter.test_normal import TestNormal\nfrom manipulation.exercises.grader import Grader\n\nGrader.grade_output([TestNormal], [locals()], \"results.json\")\nGrader.print_test_results(\"results.json\")","block_group":"95b19e27806741358edffc232900816d","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"667124db99d3447698aab185a65fa978","cell_type":"markdown","metadata":{"cell_id":"667124db99d3447698aab185a65fa978","deepnote_cell_type":"markdown"},"source":"## Verification 2: Computing Surface Normals by Nearest Pixels\n\nImplement the exercises below. Copy the exact numerical values (to 4 decimal places) for your verification keys, which you can copy/paste to Gradescope.\n\n**Question:** From the list of normal-frame RigidTransforms calculated above, what is the translation component of the first one?","block_group":"780b5c6eb4d14ca2af967e0b55cc1881"},{"cellId":"ce5c3a4446574a5ab9157bf92a03e32e","cell_type":"code","metadata":{"cell_id":"ce5c3a4446574a5ab9157bf92a03e32e","deepnote_cell_type":"code"},"source":"# Uncomment this after you have implemented the method\n# normals = estimate_normal_by_nearest_pixels(X_WC, pC)\n# normals[0].translation()","block_group":"02b4ce75697c4cc9a58f5568dc7afaca","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null}],
        "metadata": {"deepnote_notebook_id":"fd547de58fec4dfdbeb5c4c52ce20564"},
        "nbformat": "4",
        "nbformat_minor": "0",
        "version": "0"
      }